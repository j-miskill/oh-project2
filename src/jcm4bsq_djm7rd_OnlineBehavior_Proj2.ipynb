{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OttCblkZ4Eh1"
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gKdwN4RstlI",
    "outputId": "2f16e511-e594-4a45-e631-6815ab4bd9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: googlemaps in /usr/local/lib/python3.11/dist-packages (4.10.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from googlemaps) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2025.1.31)\n",
      "Collecting haversine\n",
      "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install Necessary packages\n",
    "!pip install geopy\n",
    "!pip install googlemaps\n",
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQuSBmYnVuWF"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from sklearn.cluster import DBSCAN\n",
    "import googlemaps\n",
    "from haversine import haversine_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzFC_OM64JtY"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNMQLZSzYHCR",
    "outputId": "cefc0f66-6678-4fe7-8572-bee42e0b83f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                Start_Time                  End_Time  \\\n",
      "0  2025-02-06  2025-02-06 | 03:20:48 PM  2025-02-07 | 10:32:51 PM   \n",
      "1  2025-02-07  2025-02-07 | 10:32:51 PM  2025-02-07 | 10:44:19 PM   \n",
      "2  2025-02-07  2025-02-07 | 10:44:19 PM  2025-02-08 | 12:40:56 PM   \n",
      "3  2025-02-08  2025-02-08 | 12:40:56 PM  2025-02-08 | 01:11:02 PM   \n",
      "4  2025-02-08  2025-02-08 | 01:11:02 PM  2025-02-08 | 01:14:01 PM   \n",
      "\n",
      "   Duration (minutes)     Event            Event_Type Start_Location  \\\n",
      "0         1872.050333     visit                 visit           None   \n",
      "1           11.482967  activity  in passenger vehicle           None   \n",
      "2          836.600017  activity  in passenger vehicle           None   \n",
      "3           30.100000  activity  in passenger vehicle           None   \n",
      "4            2.983333     visit                 visit           None   \n",
      "\n",
      "  End_Location                         start_dt  \\\n",
      "0         None 2025-02-06 15:20:48.001000+00:00   \n",
      "1         None 2025-02-07 22:32:51.021000+00:00   \n",
      "2         None 2025-02-07 22:44:19.999000+00:00   \n",
      "3         None        2025-02-08 12:40:56+00:00   \n",
      "4         None        2025-02-08 13:11:02+00:00   \n",
      "\n",
      "                            end_dt  start_lat  start_long    end_lat  \\\n",
      "0 2025-02-07 22:32:51.021000+00:00  38.057952  -78.493924  38.057952   \n",
      "1 2025-02-07 22:44:19.999000+00:00  38.029188  -78.474189  38.060148   \n",
      "2        2025-02-08 12:40:56+00:00  38.041748  -79.124244  37.864687   \n",
      "3        2025-02-08 13:11:02+00:00  37.658793  -79.533981  37.499744   \n",
      "4        2025-02-08 13:14:01+00:00  37.499883  -79.750426  37.499883   \n",
      "\n",
      "    end_long  \n",
      "0 -78.493924  \n",
      "1 -78.502100  \n",
      "2 -79.338281  \n",
      "3 -79.749278  \n",
      "4 -79.750426  \n",
      "Index(['Date', 'Start_Time', 'End_Time', 'Duration (minutes)', 'Event',\n",
      "       'Event_Type', 'Start_Location', 'End_Location', 'start_dt', 'end_dt',\n",
      "       'start_lat', 'start_long', 'end_lat', 'end_long'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load Google Timeline JSON Data\n",
    "records = []\n",
    "with open('google-timeline.json') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "    # Parse JSON and get relevant information\n",
    "    for entry in raw_data:\n",
    "\n",
    "        start_time = parser.parse(entry[\"startTime\"])\n",
    "        end_time = parser.parse(entry[\"endTime\"])\n",
    "\n",
    "        if \"activity\" in entry:\n",
    "            event = \"activity\"\n",
    "            event_type = entry[\"activity\"][\"topCandidate\"][\"type\"]\n",
    "            start_lat, start_lon = map(float, entry[\"activity\"][\"start\"][4:].split(\",\"))\n",
    "            end_lat, end_lon = map(float, entry[\"activity\"][\"end\"][4:].split(\",\"))\n",
    "        elif \"visit\" in entry:\n",
    "            event = \"visit\"\n",
    "            event_type = \"visit\"\n",
    "            start_lat, start_lon = map(float, entry[\"visit\"][\"topCandidate\"][\"placeLocation\"][4:].split(\",\"))\n",
    "            end_lat, end_lon = start_lat, start_lon  # Visits have the same start and end\n",
    "\n",
    "        # Initialize location columns for later\n",
    "        start_location, end_location = None, None\n",
    "\n",
    "        records.append({\n",
    "            \"Date\": start_time.date(),\n",
    "            \"start_dt\": start_time,\n",
    "            \"end_dt\": end_time,\n",
    "            \"Duration (minutes)\": (end_time - start_time).total_seconds() / 60,  # in minutes\n",
    "            \"Event\": event,\n",
    "            \"Event_Type\": event_type,\n",
    "            \"start_lat\": start_lat,\n",
    "            \"start_long\": start_lon,\n",
    "            \"Start_Location\": start_location,\n",
    "            \"end_lat\": end_lat,\n",
    "            \"end_long\": end_lon,\n",
    "            \"End_Location\": end_location\n",
    "        })\n",
    "\n",
    "# Convert parsed data into dataframe\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Ensure datetime conversion\n",
    "df[\"start_dt\"] = pd.to_datetime(df[\"start_dt\"], errors=\"coerce\", utc=True)\n",
    "df[\"end_dt\"] = pd.to_datetime(df[\"end_dt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Now you can create formatted columns\n",
    "df[\"Start_Time\"] = df[\"start_dt\"].dt.strftime(\"%Y-%m-%d | %I:%M:%S %p\")\n",
    "df[\"End_Time\"] = df[\"end_dt\"].dt.strftime(\"%Y-%m-%d | %I:%M:%S %p\")\n",
    "\n",
    "# Reorder DF columns\n",
    "df = df[[\"Date\", \"Start_Time\", \"End_Time\", \"Duration (minutes)\", \"Event\", \"Event_Type\", \"Start_Location\", \"End_Location\",\n",
    "         \"start_dt\", \"end_dt\", \"start_lat\", \"start_long\",  \"end_lat\", \"end_long\"]]\n",
    "\n",
    "# Write DF to csv\n",
    "df.to_csv(\"google-timeline-without-locations.csv\", index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJ2N95u8lN0X",
    "outputId": "728b8451-d73c-44a1-b4da-24eb9cbda70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Start_Location  \\\n",
      "0      1600 Emmet St N, Charlottesville, VA 22901, USA   \n",
      "1      923 E Market St, Charlottesville, VA 22902, USA   \n",
      "2         44 Swartzel Shop Rd, Staunton, VA 24401, USA   \n",
      "3      1085 Red Mill Rd, Natural Bridge, VA 24578, USA   \n",
      "4               15162 Lee Hwy, Buchanan, VA 24066, USA   \n",
      "..                                                 ...   \n",
      "885  2402 Smithfield Rd, Charlottesville, VA 22901,...   \n",
      "886  2402 Smithfield Rd, Charlottesville, VA 22901,...   \n",
      "887  2402 Smithfield Rd, Charlottesville, VA 22901,...   \n",
      "888  2402 Smithfield Rd, Charlottesville, VA 22901,...   \n",
      "889  2402 Smithfield Rd, Charlottesville, VA 22901,...   \n",
      "\n",
      "                                          End_Location  \n",
      "0      1600 Emmet St N, Charlottesville, VA 22901, USA  \n",
      "1        1601 Ricky Dr, Charlottesville, VA 22901, USA  \n",
      "2                       I-81, Fairfield, VA 24435, USA  \n",
      "3               15166 Lee Hwy, Buchanan, VA 24066, USA  \n",
      "4               15162 Lee Hwy, Buchanan, VA 24066, USA  \n",
      "..                                                 ...  \n",
      "885  85 Engineer's Way, Charlottesville, VA 22903, USA  \n",
      "886  85 Engineer's Way, Charlottesville, VA 22903, USA  \n",
      "887  85 Engineer's Way, Charlottesville, VA 22903, USA  \n",
      "888  85 Engineer's Way, Charlottesville, VA 22903, USA  \n",
      "889  85 Engineer's Way, Charlottesville, VA 22903, USA  \n",
      "\n",
      "[890 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read Data without locations file\n",
    "df = pd.read_csv(\"data/google-timeline-without-locations.csv\")\n",
    "\n",
    "# Initialize Google Maps client\n",
    "geolocator = googlemaps.Client(key=\"<GOOGLE MAPS API KEY>\")\n",
    "\n",
    "# Dictionary to store cached results\n",
    "location_cache = {}\n",
    "\n",
    "# Function to get location name from latitude and longitude\n",
    "def get_location(lat, lon):\n",
    "\n",
    "    key = (lat, lon)\n",
    "    if key in location_cache:\n",
    "        return location_cache[key]  # Use cached result\n",
    "\n",
    "    time.sleep(0.1)  # Delaying to avoid rate limits\n",
    "    if pd.notna(lat) and pd.notna(lon):  # Ensure coordinates exist\n",
    "        try:\n",
    "          result = geolocator.reverse_geocode((lat, lon))\n",
    "          if result:\n",
    "              address = result[0][\"formatted_address\"]\n",
    "              location_cache[key] = address  # Store in cache\n",
    "              return address\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Apply function to fill Start_Location and End_Location\n",
    "df[\"Start_Location\"] = df.apply(lambda row: get_location(row[\"start_lat\"], row[\"start_long\"]), axis=1)\n",
    "df[\"End_Location\"] = df.apply(lambda row: get_location(row[\"end_lat\"], row[\"end_long\"]), axis=1)\n",
    "\n",
    "# Print updated DataFrame\n",
    "print(df[[\"Start_Location\", \"End_Location\"]])\n",
    "\n",
    "# Save df to csv\n",
    "df.to_csv(\"google-timeline-with-locations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ZEyb0S5kWJ"
   },
   "source": [
    "## Data Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eCr5LgpBoR3"
   },
   "outputs": [],
   "source": [
    "# Read CSV File\n",
    "df = pd.read_csv(\"data/google-timeline-with-locations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Mehw-rNDYQsd"
   },
   "outputs": [],
   "source": [
    "# Function to cluster locations considering both start & end points\n",
    "def cluster_events(df, eps_meters=100, time_threshold=\"30min\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure datetime conversion\n",
    "    df[\"start_dt\"] = pd.to_datetime(df[\"start_dt\"], errors=\"coerce\", utc=True)\n",
    "    df[\"end_dt\"] = pd.to_datetime(df[\"end_dt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Convert coordinates to numeric\n",
    "    df[\"start_lat\"] = pd.to_numeric(df[\"start_lat\"], errors=\"coerce\")\n",
    "    df[\"start_long\"] = pd.to_numeric(df[\"start_long\"], errors=\"coerce\")\n",
    "    df[\"end_lat\"] = pd.to_numeric(df[\"end_lat\"], errors=\"coerce\")\n",
    "    df[\"end_long\"] = pd.to_numeric(df[\"end_long\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with NaN values in essential columns\n",
    "    df = df.dropna(subset=[\"start_lat\", \"start_long\", \"end_lat\", \"end_long\", \"start_dt\"])\n",
    "\n",
    "    ## --------- 1. Location Clustering (Considering Start & End) ---------\n",
    "    start_coords = np.radians(df[[\"start_lat\", \"start_long\"]].values)\n",
    "    end_coords = np.radians(df[[\"end_lat\", \"end_long\"]].values)\n",
    "\n",
    "    # Compute pairwise Haversine distance for both start and end points\n",
    "    start_dist = haversine_vector(start_coords, start_coords, comb=True) * 1000  # Convert to meters\n",
    "    end_dist = haversine_vector(end_coords, end_coords, comb=True) * 1000  # Convert to meters\n",
    "\n",
    "    # Combine start and end distances (weighted sum or max distance)\n",
    "    combined_dist = np.maximum(start_dist, end_dist)  # Conservative approach: take the max distance\n",
    "\n",
    "    loc_db = DBSCAN(eps=eps_meters, min_samples=1, metric=\"precomputed\").fit(combined_dist)\n",
    "    df[\"location_cluster\"] = loc_db.labels_  # Assign spatial clusters\n",
    "\n",
    "    ## --------- 2. Time Clustering (Sequential Check) ---------\n",
    "    df = df.sort_values(by=[\"start_dt\"]).reset_index(drop=True)  # Sort by time\n",
    "    time_threshold_sec = pd.to_timedelta(time_threshold).total_seconds()\n",
    "\n",
    "    time_cluster = -1\n",
    "    prev_time = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if prev_time is None or (df.loc[i, \"start_dt\"] - prev_time).total_seconds() > time_threshold_sec:\n",
    "            time_cluster += 1  # New cluster when time gap is large\n",
    "        df.loc[i, \"time_cluster\"] = time_cluster\n",
    "        prev_time = df.loc[i, \"start_dt\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "clustered_df = cluster_events(df)\n",
    "\n",
    "# Write data to csv\n",
    "clustered_df.to_csv(\"google-timeline-with-locations-clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6ureosCdyC9"
   },
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "clustered_df = pd.read_csv(\"data/google-timeline-with-locations-clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "-ylmXSTpLOMK"
   },
   "outputs": [],
   "source": [
    "# Initialize Google Maps client\n",
    "geolocator = googlemaps.Client(key=\"<GOOGLE API KEY>\")\n",
    "\n",
    "# Function to get Place ID using Geocoding API\n",
    "def get_place_id(address):\n",
    "    try:\n",
    "        geocode_result = geolocator.geocode(address)\n",
    "        if geocode_result:\n",
    "            return geocode_result[0].get('place_id', None)\n",
    "        return None\n",
    "    except GeocoderTimedOut:\n",
    "        return None\n",
    "\n",
    "# Function to get business/building name using Places API\n",
    "def get_business_name(place_id):\n",
    "    try:\n",
    "        if place_id:\n",
    "            place_details = geolocator.place(place_id)\n",
    "            if 'result' in place_details:\n",
    "                return place_details['result'].get('name', \"Unknown\")\n",
    "        return \"Unknown\"\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Timeout\"\n",
    "\n",
    "# Function to get the business name based on start and end locations\n",
    "def get_location_name(address):\n",
    "    place_id = get_place_id(address)\n",
    "    return get_business_name(place_id)\n",
    "\n",
    "# Apply reverse geocoding to get location labels and business/building names\n",
    "clustered_df[\"Start_Location_Name\"] = clustered_df[\"Start_Location\"].apply(get_location_name)\n",
    "clustered_df[\"End_Location_Name\"] = clustered_df[\"End_Location\"].apply(get_location_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "A9rlXmFzUF_b"
   },
   "outputs": [],
   "source": [
    "# Reorder DF columns\n",
    "clustered_df = clustered_df[[\"Date\", \"Start_Time\", \"End_Time\", \"Duration (minutes)\", \"Event\", \"Event_Type\", \"Start_Location\", \"Start_Location_Name\", \"End_Location\",\n",
    "         \"End_Location_Name\", \"start_dt\", \"end_dt\", \"start_lat\", \"start_long\",  \"end_lat\", \"end_long\", \"location_cluster\", \"time_cluster\"]]\n",
    "\n",
    "# Save the updated dataframe\n",
    "clustered_df.to_csv(\"labeled-google-timeline-with-locations-clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSDOZ8MZSnxq"
   },
   "source": [
    "## Data Analysis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OttCblkZ4Eh1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
